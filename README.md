# WaChat

‚ú® O que voc√™ √©

Um espa√ßo seguro de escuta espiritual, com reflex√µes crist√£s e respostas personalizadas via Telegram, sem julgamento.

Frase-guia interna

‚ÄúN√£o te digo o que pensar. Caminho contigo enquanto voc√™ pensa.‚Äù



postgres hosted at: https://supabase.com/dashboard/project/

WaChat √© um projeto Django para aplica√ß√£o de chat via Telegram.

cloudflared tunnel --url http://localhost:9000

## üìã Requisitos

- Python 3.8+
- PostgreSQL (ou SQLite para desenvolvimento)

## üöÄ Instala√ß√£o

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/avictorino/wachat.git
cd wachat
```

### 2. Crie e ative um ambiente virtual

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

### 3. Instale as depend√™ncias

```bash
pip install -r requirements.txt
```

### 4. Configure as vari√°veis de ambiente

Copie o arquivo `.env.example` para `.env` e ajuste as configura√ß√µes:

```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas configura√ß√µes:

```env
# Django Settings
SECRET_KEY=sua-chave-secreta-aqui
DEBUG=True
ALLOWED_HOSTS=localhost,127.0.0.1

# Database Configuration
# Para PostgreSQL:
DATABASE_URL=postgresql://usuario:senha@localhost:5432/nome_do_banco

# Para SQLite (desenvolvimento):
# DATABASE_URL=sqlite:///db.sqlite3
```

**Importante:** O arquivo `.env` n√£o ser√° versionado (j√° est√° no `.gitignore`) para proteger suas credenciais.

### 5. Execute as migra√ß√µes

```bash
python manage.py migrate
```

### 6. Crie um superusu√°rio (opcional)

```bash
python manage.py createsuperuser
```

### 7. Inicie o servidor de desenvolvimento

```bash
python manage.py runserver
```

Acesse a aplica√ß√£o em: http://localhost:8000

## üóÑÔ∏è Configura√ß√£o do Banco de Dados

Este projeto utiliza `dj-database-url` para simplificar a configura√ß√£o do banco de dados atrav√©s de uma URL string.

### PostgreSQL (Produ√ß√£o/Desenvolvimento)

No arquivo `.env`, configure a vari√°vel `DATABASE_URL`:

```env
DATABASE_URL=postgresql://usuario:senha@host:porta/nome_do_banco
```

Exemplos:
- Local: `postgresql://postgres:senha123@localhost:5432/wachat`
- Heroku: `postgresql://user:pass@ec2-xxx.compute.amazonaws.com:5432/dbname`

### SQLite (Desenvolvimento Local)

Se preferir usar SQLite para desenvolvimento local, deixe a vari√°vel `DATABASE_URL` vazia ou use:

```env
DATABASE_URL=sqlite:///db.sqlite3
```

## üì¶ Depend√™ncias

- **Django 4.2.27**: Framework web principal
- **dj-database-url**: Configura√ß√£o de banco de dados via URL
- **python-decouple**: Gerenciamento de vari√°veis de ambiente
- **django-dotenv**: Carregamento autom√°tico de vari√°veis do arquivo .env
- **psycopg2-binary**: Driver PostgreSQL

## üõ†Ô∏è Desenvolvimento

### Estrutura do Projeto

```
wachat/
‚îú‚îÄ‚îÄ core/               # App principal
‚îú‚îÄ‚îÄ config/            # Configura√ß√µes do projeto
‚îÇ   ‚îú‚îÄ‚îÄ settings.py    # Configura√ß√µes Django
‚îÇ   ‚îú‚îÄ‚îÄ urls.py        # URLs principais
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py        # WSGI config
‚îú‚îÄ‚îÄ manage.py          # Utilit√°rio Django
‚îú‚îÄ‚îÄ requirements.txt   # Depend√™ncias Python
‚îú‚îÄ‚îÄ .env.example       # Exemplo de vari√°veis de ambiente
‚îî‚îÄ‚îÄ README.md         # Este arquivo
```

### Comandos √öteis

```bash
# Criar migra√ß√µes
python manage.py makemigrations

# Aplicar migra√ß√µes
python manage.py migrate

# Rodar servidor
python manage.py runserver

# Criar superusu√°rio
python manage.py createsuperuser

# Rodar testes
python manage.py test

# Coletar arquivos est√°ticos
python manage.py collectstatic

# Simular conversa realista entre humano e bot
python manage.py simulate_conversation --turns 5 --domain spiritual

# Simular conversa entre dois agentes de IA (buscador e ouvinte)
python manage.py simulate --num-messages 8
```

## ü§ñ Simula√ß√£o de Conversas

O projeto inclui dois comandos de gerenciamento para simular conversas:

### `simulate_conversation` - Simula√ß√£o Realista com Humano

Simula uma conversa realista entre um usu√°rio humano (simulado por IA) e o bot. Isso √© √∫til para:

- Testar o fluxo completo de conversa√ß√£o
- Validar progress√£o do funil e gerenciamento de estado
- Gerar dados de teste para desenvolvimento
- Demonstrar capacidades conversacionais do bot

```bash
# Simula√ß√£o b√°sica com 5 turnos
python manage.py simulate_conversation

# Simula√ß√£o personalizada
python manage.py simulate_conversation --turns 10 --domain grief --name "Ana Costa"

# Modo de teste (sem chamadas reais de API)
python manage.py simulate_conversation --mock-telegram --turns 3
```

Para documenta√ß√£o completa, veja [docs/SIMULATE_CONVERSATION.md](docs/SIMULATE_CONVERSATION.md).

### `simulate` - Simula√ß√£o entre Dois Agentes de IA

Simula uma conversa entre dois agentes de IA (buscador e ouvinte) e fornece an√°lise cr√≠tica. √ötil para:

- Testar a qualidade do di√°logo do bot
- Avaliar a empatia e resposta do ouvinte
- Analisar verbosidade e interpreta√ß√£o das respostas
- Gerar exemplos de conversas para treinamento

```bash
# Simula√ß√£o b√°sica com 8 mensagens
python manage.py simulate

# Simula√ß√£o com n√∫mero personalizado de mensagens (6-10)
python manage.py simulate --num-messages 10

# Modo silencioso (apenas a conversa e an√°lise)
python manage.py simulate --quiet
```

O comando gera uma conversa alternada entre:
- üßë‚Äçüí¨ **Pessoa** (ROLE_A): pessoa em busca espiritual, vulner√°vel e cautelosa
- üåø **BOT** (ROLE_B): assistente emp√°tico e n√£o-julgador

Ao final, exibe uma an√°lise cr√≠tica em 5 se√ß√µes:
1. O que funcionou bem
2. Pontos de poss√≠vel erro de interpreta√ß√£o
3. Problemas de verbosidade e extens√£o das respostas
4. O que poderia ter sido feito diferente
5. Ajustes recomendados para pr√≥ximas intera√ß√µes

## üí¨ Comandos do Telegram Bot

Os seguintes comandos est√£o dispon√≠veis no bot do Telegram:

### `/start`
Inicia uma nova conversa com o bot. Cria um perfil de usu√°rio, infere g√™nero a partir do nome e envia uma mensagem de boas-vindas personalizada.

### `/reset`
Inicia o processo de exclus√£o de dados do usu√°rio. Solicita confirma√ß√£o antes de deletar permanentemente o perfil, conversas e mensagens. O usu√°rio deve responder com "CONFIRM" dentro de 5 minutos.

### `/simulate [n√∫mero]`
**Novo!** Executa uma simula√ß√£o completa de conversa entre dois pap√©is de IA:
- üßë‚Äçüí¨ **Pessoa**: Uma pessoa em busca espiritual, vulner√°vel e questionadora
- üåø **BOT**: Um assistente espiritual emp√°tico e n√£o-julgador

**Uso:**
- `/simulate` - Gera 8 mensagens (padr√£o)
- `/simulate 6` - Gera 6 mensagens (m√≠nimo)
- `/simulate 10` - Gera 10 mensagens (m√°ximo)

O comando gera o n√∫mero especificado de mensagens alternadas (6-10, padr√£o 8), persiste tudo no banco de dados, e retorna:
1. Cada mensagem da conversa simulada com identifica√ß√£o de papel
2. Uma an√°lise cr√≠tica final da conversa, incluindo:
   - O que funcionou bem
   - Pontos de poss√≠vel erro de interpreta√ß√£o
   - Problemas de verbosidade e extens√£o das respostas
   - O que poderia ter sido feito diferente
   - Ajustes recomendados para pr√≥ximas intera√ß√µes

**√ötil para:**
- Demonstrar as capacidades do bot
- Testar o fluxo conversacional
- Visualizar an√°lise cr√≠tica em a√ß√£o
- Gerar exemplos de conversas

## üìù Vari√°veis de Ambiente

| Vari√°vel | Descri√ß√£o | Padr√£o | Obrigat√≥ria |
|----------|-----------|--------|-------------|
| `SECRET_KEY` | Chave secreta do Django | - | Sim (produ√ß√£o) |
| `DEBUG` | Modo de debug | `True` | N√£o |
| `ALLOWED_HOSTS` | Hosts permitidos (separados por v√≠rgula) | - | Sim (produ√ß√£o) |
| `DATABASE_URL` | URL de conex√£o com o banco de dados | SQLite local | N√£o |
| `LLM_PROVIDER` | Provedor de LLM (`groq` ou `ollama`) | `groq` | N√£o |
| `GROQ_API_KEY` | Chave da API Groq | - | Sim (se LLM_PROVIDER=groq) |
| `OLLAMA_BASE_URL` | URL base do servidor Ollama local | `http://localhost:11434` | N√£o |
| `OLLAMA_MODEL` | Modelo Ollama a ser usado | `llama3.1` | N√£o |

## ü§ñ Configura√ß√£o do Provedor de LLM

O WaChat suporta dois provedores de LLM (Large Language Model):

### 1. Groq (Padr√£o - Cloud API)

O Groq √© o provedor padr√£o e utiliza a API cloud da Groq.

**Configura√ß√£o:**
```env
LLM_PROVIDER=groq
GROQ_API_KEY=sua-chave-api-groq
```

**Pr√≥s:**
- Setup simples (apenas API key)
- Alta performance
- Sem necessidade de hardware local

**Contras:**
- Requer chave de API
- Custos por uso (dependendo do plano)
- Requer conex√£o com internet

### 2. Ollama (Local)

O Ollama permite executar modelos LLM localmente, sem depend√™ncia de APIs externas.

**Configura√ß√£o:**

1. **Instale o Ollama:**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Ou visite: https://ollama.com/download
   ```

2. **Baixe um modelo:**
   ```bash
   # Recomendado: llama3.1 (modelo padr√£o)
   ollama pull llama3.1
   
   # Ou outros modelos:
   # ollama pull llama3
   # ollama pull mistral
   # ollama pull codellama
   ```

3. **Crie um modelo customizado a partir do Modelfile (Opcional):**
   
   O WaChat inclui um `Modelfile` na raiz do projeto que define o comportamento
   conversacional base do assistente. Para usar o Ollama com este comportamento
   customizado, crie um modelo do Ollama a partir do Modelfile:
   
   ```bash
   # Na raiz do projeto wachat
   ollama create wachat -f Modelfile
   
   # Configure o modelo no .env
   OLLAMA_MODEL=wachat
   ```
   
   **Nota:** Esta etapa √© **recomendada** para melhor experi√™ncia com Ollama.
   O Modelfile define o comportamento conversacional completo do assistente, incluindo
   tom, regras de conversa√ß√£o e postura espiritual. O c√≥digo da aplica√ß√£o envia apenas
   instru√ß√µes din√¢micas e contextuais (temas e modos de resposta).

4. **Inicie o servidor Ollama:**
   ```bash
   ollama serve
   # O servidor ser√° iniciado em http://localhost:11434
   ```

5. **Configure as vari√°veis de ambiente:**
   ```env
   LLM_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434  # Padr√£o, pode ser omitido
   OLLAMA_MODEL=llama3.1                   # Padr√£o, pode ser omitido (ou use 'wachat' se criou o modelo customizado)
   ```

6. **Inicie o WaChat:**
   ```bash
   python manage.py runserver
   ```

**Pr√≥s:**
- Totalmente local (sem custos de API)
- Privacidade completa dos dados
- Sem limita√ß√µes de tokens
- Funciona offline

**Contras:**
- Requer hardware adequado (GPU recomendada)
- Setup inicial mais complexo
- Pode ser mais lento que APIs cloud

**Modelos Recomendados:**
- `llama3.1` (padr√£o) - Bom equil√≠brio entre qualidade e performance
- `llama3` - Alternativa mais leve
- `mistral` - Outra op√ß√£o de qualidade
- `gemma` - Modelo do Google, tamb√©m eficiente

**Exemplo de uso com modelo customizado:**
```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral  # Usando Mistral em vez do padr√£o
```

### Alternando entre provedores

Voc√™ pode facilmente alternar entre provedores mudando a vari√°vel `LLM_PROVIDER`:

```bash
# Usar Groq
export LLM_PROVIDER=groq
export GROQ_API_KEY=sua-chave

# Ou usar Ollama
export LLM_PROVIDER=ollama
```

A aplica√ß√£o detectar√° automaticamente o provedor configurado e utilizar√° o servi√ßo apropriado sem necessidade de mudan√ßas no c√≥digo.

## üîí Seguran√ßa

- **NUNCA** commite o arquivo `.env` no reposit√≥rio
- Mantenha o `SECRET_KEY` seguro e √∫nico por ambiente
- Em produ√ß√£o, sempre configure `DEBUG=False`
- Configure `ALLOWED_HOSTS` apropriadamente em produ√ß√£o

## üöÄ Deploy para Heroku

Para instru√ß√µes completas de deployment no Heroku, consulte o [Guia de Deploy para Heroku](HEROKU_DEPLOYMENT.md).

O deploy no Heroku inclui:
- Configura√ß√£o autom√°tica de PostgreSQL
- Execu√ß√£o autom√°tica de migrations durante o deploy
- Sincroniza√ß√£o com o branch `main` do GitHub
- Python buildpack configurado
- Gunicorn como servidor WSGI

## üìÑ Licen√ßa

[Especifique a licen√ßa do projeto aqui]

## üë• Contribui√ß√£o

[Instru√ß√µes para contribuir com o projeto]

## üìß Contato

[Informa√ß√µes de contato]
